{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNjoiJv47G9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import librosa as lbrs\n",
        "import wave as wv\n",
        "import glob\n",
        "import audioread as ar\n",
        "import binascii\n",
        "import sys\n",
        "import random\n",
        "from urllib.request import urlretrieve\n",
        "import os\n",
        "import tarfile\n",
        "import pickle\n",
        "import random\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vYJly4v-Aey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download(url, file):\n",
        "    if not os.path.isfile(file):\n",
        "        print(\"Download file... \" + file + \" ...\")\n",
        "        urlretrieve(url,file)\n",
        "        print(\"File downloaded\")\n",
        "\n",
        "def uncompress(fname):\n",
        "    tar = tarfile.open(fname, \"r:gz\")\n",
        "    tar.extractall()\n",
        "    tar.close()\n",
        "    \n",
        "def indices_to_one_hot(dta):\n",
        "  one_hot = np.zeros((len(dta), len(np.unique(dta))), np.uint8)\n",
        "  one_hot[np.arange(len(dta)), dta] = 1\n",
        "  return one_hot\n",
        "\n",
        "def read_audio_file(file):\n",
        "    file_data = np.zeros(sample_length, np.uint8)\n",
        "    with ar.audio_open(audio_file_path) as audio_file:\n",
        "      count = 0\n",
        "      sys.stdout.write('.')\n",
        "      for buf in audio_file:\n",
        "        for sample in buf:\n",
        "          if count >= sample_length:\n",
        "            break\n",
        "          file_data[count] = sample\n",
        "          count = count + 1\n",
        "        if count >= sample_length:\n",
        "            break\n",
        "    return file_data\n",
        "  \n",
        "def getbatch(alldata, alllabels, batch_size = 16):\n",
        "    count = 0\n",
        "    newdata = alldata.copy()\n",
        "    arraylength = np.ma.size(newdata, axis=0)\n",
        "    while count < arraylength/batch_size:\n",
        "        randstart = random.randint(0, arraylength-batch_size-1)\n",
        "        count += 1\n",
        "        x = newdata[randstart:randstart+batch_size]\n",
        "        yield (x, alllabels[randstart:randstart+batch_size])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IsAuyUj_feZ",
        "colab_type": "code",
        "outputId": "60710343-6f8c-4218-861e-335f3696ba58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "download('http://opihi.cs.uvic.ca/sound/genres.tar.gz', 'genres.tar.gz')\n",
        "if not os.path.isdir('genres'):\n",
        "  uncompress('genres.tar.gz')\n",
        "!rm genres.tar.gz"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Download file... genres.tar.gz ...\n",
            "File downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7vZNSgegIzE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Alex CNN hyper-parameters\n",
        "CNN_nb_categories = 10\n",
        "CNN_nb_files_per_cat = 100\n",
        "CNN_total_files = CNN_nb_categories * CNN_nb_files_per_cat\n",
        "CNN_genres_map = {}\n",
        "CNN_genres_map_reverse = {}\n",
        "\n",
        "#CNN_noofdatapoints = 10000\n",
        "\n",
        "CNN_learningrate = 0.001\n",
        "CNN_nepochs = 10\n",
        "CNN_batch_size = 5\n",
        "CNN_noofbatches = 10\n",
        "\n",
        "#CNN_n_input = 784 # 28x28 image\n",
        "CNN_n_classes = 10 # 1 for each digit [0-9]\n",
        "CNN_dropout = 0.75\n",
        "\n",
        "CNN_data_max_size_div = 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er-MxwuCpqyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Alex CNN_utilities_functions\n",
        "\"\"\"\n",
        "FUNCTION NAME : CNN_find_file\n",
        "INPUT : number :int, directory :String\n",
        "OUTPUT : :String\n",
        "DESCRIPTION : returns the path corresponding to the n-1 element in the directory passed as input\n",
        "\"\"\"\n",
        "def CNN_find_file(number, directory):\n",
        "  return(glob.glob(directory)[number-1])\n",
        "\n",
        "\"\"\"\n",
        "FUNCTION NAME : CNN_one_hot\n",
        "INPUT : data : list<(list<int>, string)>\n",
        "OUTPUT : CNN_one_hot : ndarray\n",
        "DESCRIPTION : create the one-hot matrix for the batch of data\n",
        "\"\"\"\n",
        "def CNN_one_hot(data):\n",
        "  nb_files = len(data)\n",
        "  CNN_one_hot = np.zeros((nb_files, CNN_nb_categories))\n",
        "  count = 0\n",
        "  for element in data:\n",
        "    CNN_one_hot[count, CNN_genres_map_reverse[element[1]]] = 1\n",
        "    count+=1\n",
        "  return CNN_one_hot\n",
        "\n",
        "def CNN_truncate_data(data, devider):\n",
        "  new_length = len(data)//devider\n",
        "  return data[0:new_length]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRt27b4mFMmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Alex CNN_step_1\n",
        "\"\"\"\n",
        "FUNCTION NAME : CNN_get_batches\n",
        "INPUT : CNN_nb_files_per_cat_wanted : int\n",
        "OUTPUT : CNN_files_to_read : list<String>\n",
        "DESCRIPTION : returns a list that contains the names of n files per categories, n given as an input\n",
        "\"\"\"\n",
        "def CNN_get_batches(CNN_nb_files_per_cat_wanted = 20):\n",
        "  CNN_files_to_read = []\n",
        "  CNN_count = 0\n",
        "  CNN_count_file = 0\n",
        "  \n",
        "  for CNN_dir_name in glob.glob(\"genres/*\"):\n",
        "    CNN_genres = CNN_dir_name.split(\"/\")\n",
        "    CNN_genres = CNN_genres[len(CNN_genres)-1]\n",
        "    CNN_genres_map[CNN_count] = CNN_genres\n",
        "    CNN_genres_map_reverse[CNN_genres] = CNN_count\n",
        "    #Get the batches\n",
        "    CNN_rand_numbers = [x for x in range(CNN_nb_files_per_cat)]\n",
        "    random.shuffle(CNN_rand_numbers)\n",
        "    CNN_rand_numbers = CNN_rand_numbers[0:CNN_nb_files_per_cat_wanted]\n",
        "    #print(CNN_rand_numbers)\n",
        "    CNN_f = [CNN_find_file(x, CNN_dir_name + \"/*\") for x in CNN_rand_numbers]\n",
        "    CNN_files_to_read.extend(CNN_f)\n",
        "    #print(CNN_files_to_read, len(CNN_files_to_read))\n",
        "    #print(len(set(CNN_files_to_read)), len(set(CNN_files_to_read)) == len(CNN_files_to_read))\n",
        "    \"\"\"if CNN_count>1:\n",
        "      break\"\"\"\n",
        "    CNN_count+=1\n",
        "  return CNN_files_to_read"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMB5yIhNKhjG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Alex CNN_step_2\n",
        "\"\"\"\n",
        "FUNCTION NAME : CNN_read_batch\n",
        "INPUT : file_list : list<String>\n",
        "OUTPUT : CNN_data : ndarray\n",
        "DESCRIPTION : returns the array containing the data read from the files provided as input\n",
        "\"\"\"\n",
        "def CNN_read_batch(file_list, devider):\n",
        "  alldata = []\n",
        "  for file in file_list:\n",
        "    #print(file, type(file))\n",
        "    genre = file.split(\"/\")\n",
        "    genre = genre[len(genre)-2]\n",
        "    with ar.audio_open(file) as f:\n",
        "      data = []\n",
        "      for buf in f:\n",
        "        for buf_element in buf:\n",
        "          data.append(buf_element)\n",
        "      #print(len(data), type(data),genre)\n",
        "      data = CNN_truncate_data(data, devider)\n",
        "      alldata.append((data,genre))\n",
        "    #break\n",
        "  return alldata"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLrOh0NhQN83",
        "colab_type": "code",
        "outputId": "93ae69b9-4f77-4a75-b13f-65f0f5a896a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "test = CNN_get_batches(CNN_batch_size)\n",
        "print(test, len(test))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['genres/rock/rock.00039.au', 'genres/rock/rock.00026.au', 'genres/rock/rock.00066.au', 'genres/rock/rock.00009.au', 'genres/rock/rock.00051.au', 'genres/metal/metal.00066.au', 'genres/metal/metal.00038.au', 'genres/metal/metal.00042.au', 'genres/metal/metal.00003.au', 'genres/metal/metal.00024.au', 'genres/hiphop/hiphop.00095.au', 'genres/hiphop/hiphop.00034.au', 'genres/hiphop/hiphop.00040.au', 'genres/hiphop/hiphop.00084.au', 'genres/hiphop/hiphop.00059.au', 'genres/country/country.00054.au', 'genres/country/country.00000.au', 'genres/country/country.00037.au', 'genres/country/country.00010.au', 'genres/country/country.00079.au', 'genres/classical/classical.00027.au', 'genres/classical/classical.00083.au', 'genres/classical/classical.00004.au', 'genres/classical/classical.00041.au', 'genres/classical/classical.00060.au', 'genres/jazz/jazz.00055.au', 'genres/jazz/jazz.00010.au', 'genres/jazz/jazz.00040.au', 'genres/jazz/jazz.00013.au', 'genres/jazz/jazz.00015.au', 'genres/reggae/reggae.00059.au', 'genres/reggae/reggae.00060.au', 'genres/reggae/reggae.00033.au', 'genres/reggae/reggae.00056.au', 'genres/reggae/reggae.00040.au', 'genres/disco/disco.00083.au', 'genres/disco/disco.00082.au', 'genres/disco/disco.00002.au', 'genres/disco/disco.00023.au', 'genres/disco/disco.00039.au', 'genres/pop/pop.00070.au', 'genres/pop/pop.00048.au', 'genres/pop/pop.00013.au', 'genres/pop/pop.00056.au', 'genres/pop/pop.00095.au', 'genres/blues/blues.00041.au', 'genres/blues/blues.00070.au', 'genres/blues/blues.00068.au', 'genres/blues/blues.00061.au', 'genres/blues/blues.00038.au'] 50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhtuMTBxW5oQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = CNN_read_batch(test, CNN_data_max_size_div)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkeGgMTeLDfn",
        "colab_type": "code",
        "outputId": "45b1b151-2572-44a5-f7b8-0b5fe6d61ac1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#Alex CNN_step_3\n",
        "CNN_n_input = (min(map(lambda x:len(x[0]), data)))\n",
        "CNN_n_input_sqrt = math.floor(math.sqrt(CNN_n_input))\n",
        "CNN_n_input_adjusted = CNN_n_input_sqrt**2\n",
        "CNN_fcl_input = CNN_n_input_sqrt//4\n",
        "print(CNN_n_input, CNN_n_input_sqrt, CNN_n_input_adjusted, CNN_fcl_input)\n",
        "X = tf.placeholder(tf.float32, [None, CNN_n_input_adjusted])\n",
        "Y = tf.placeholder(tf.float32, [None, CNN_n_classes])\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "print(X.shape, Y.shape)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1323 36 1296 9\n",
            "(?, 1296) (?, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blNcMDylrQOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv2d(x, W, b, strides=1):\n",
        "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
        "    x = tf.nn.bias_add(x, b)\n",
        "    return tf.nn.relu(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdZKCX_xrZfN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def maxpool2d(x, k=2):\n",
        "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D5u0mthrbcQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv_net(x, weights, biases, dropout):\n",
        "    # reshape input to CNN_n_input_sqrtxCNN_n_input_sqrt size\n",
        "    print(x.shape)\n",
        "    x = tf.reshape(x, shape=[-1, CNN_n_input_sqrt, CNN_n_input_sqrt, 1])\n",
        "    print(x.shape)\n",
        "    \n",
        "    # Convolution layer 1\n",
        "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
        "    # Max pooling\n",
        "    conv1 = maxpool2d(conv1, k=2)\n",
        "\n",
        "    # Convolution layer 2\n",
        "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
        "    # Max pooling\n",
        "    conv2 = maxpool2d(conv2, k=2)\n",
        "\n",
        "    # Fully connected layer\n",
        "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
        "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
        "    fc1 = tf.nn.relu(fc1)\n",
        "    fc1 = tf.nn.dropout(fc1, dropout)\n",
        "\n",
        "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jsByJxosUQ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights = {\n",
        "    'wc1': tf.Variable(tf.random_normal([3, 3, 1, 16])),\n",
        "    'wc2': tf.Variable(tf.random_normal([3, 3, 1, 64])),\n",
        "    'wd1': tf.Variable(tf.random_normal([CNN_fcl_input*CNN_fcl_input*1, 1024])),\n",
        "    'out': tf.Variable(tf.random_normal([1024, CNN_n_classes]))\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'bc1': tf.Variable(tf.random_normal([16])),\n",
        "    'bc2': tf.Variable(tf.random_normal([64])),\n",
        "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
        "    'out': tf.Variable(tf.random_normal([CNN_n_classes]))\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS6a9XsBtQ9j",
        "colab_type": "code",
        "outputId": "da55c2a1-00a2-44d8-80cd-cc4d242c3824",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "# Create the model\n",
        "model = conv_net(X, weights, biases, keep_prob)\n",
        "print(model)\n",
        "# Define loss and optimizer\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=Y))\n",
        "train_min = tf.train.AdamOptimizer(learning_rate=CNN_learningrate).minimize(loss)\n",
        "\n",
        "# Evaluate model\n",
        "correct_model = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_model, tf.float32))\n",
        "\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "debug1\n",
            "(?, 1296)\n",
            "(?, 36, 36, 1)\n",
            "debug2\n",
            "debug3\n",
            "Tensor(\"Add_5:0\", shape=(?, 10), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N949inqo8P43",
        "colab_type": "code",
        "outputId": "aa898411-2123-4fe9-8ade-3fceaaad7c61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1610
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    \n",
        "    for epoch in range(CNN_nepochs):\n",
        "        for _ in range(CNN_noofbatches):\n",
        "            data = CNN_get_batches(CNN_batch_size)\n",
        "            data = CNN_read_batch(data, CNN_data_max_size_div)\n",
        "            batch_y = CNN_one_hot(data)\n",
        "            batch_x = [x[0] for x in data]\n",
        "            batch_x = [x[0:CNN_n_input_adjusted] for x in batch_x]\n",
        "            batch_x = np.array(batch_x)\n",
        "            batch_y = batch_y.astype(np.float32)\n",
        "            # Use training data for optimization\n",
        "            print(batch_x.shape, batch_y.shape)\n",
        "            sess.run(train_min, feed_dict={X:batch_x, Y:batch_y, keep_prob: CNN_dropout})\n",
        "        # Validate after every epoch\n",
        "        print(\"Working up to here!\")\n",
        "        data = CNN_get_batches(CNN_batch_size)\n",
        "        data = CNN_read_batch(data, CNN_data_max_size_div)\n",
        "        batch_y = CNN_one_hot(data)\n",
        "        batch_x = [x[0] for x in data]\n",
        "        batch_x = [x[0:CNN_n_input_adjusted] for x in batch_x]\n",
        "        batch_x = np.array(batch_x)\n",
        "        losscalc, accuracycalc = sess.run([loss, accuracy], \n",
        "                                          feed_dict={X:batch_x, Y:batch_y, keep_prob: 1.0})\n",
        "        print(\"Epoch: %d, Loss: %0.4f, Accuracy: %0.4f\"%(epoch, losscalc, accuracycalc))\n",
        "            \n",
        "    # When the training is complete and you are happy with the result\n",
        "    #TO_DO\n",
        "    data = CNN_get_batches(CNN_batch_size)\n",
        "    data = CNN_read_batch(data, CNN_data_max_size_div)\n",
        "    batch_y = CNN_one_hot(data)\n",
        "    batch_x = [x[0] for x in data]\n",
        "    batch_x = [x[0:CNN_n_input_adjusted] for x in batch_x]\n",
        "    batch_x = np.array(batch_x)\n",
        "    accuracycalc = sess.run(accuracy, \n",
        "                            feed_dict={X: batch_x, Y: batch_y, keep_prob: 1.0})\n",
        "    print(\"Testing accuracy: %0.4f\"%(accuracycalc))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50, 1296) (50, 10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "UnimplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnimplementedError\u001b[0m: Fused conv implementation does not support grouped convolutions for now.\n\t [[{{node Relu_7}}]]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-50f24942dba3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;31m# Use training data for optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCNN_dropout\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Validate after every epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Working up to here!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1368\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnimplementedError\u001b[0m: Fused conv implementation does not support grouped convolutions for now.\n\t [[node Relu_7 (defined at <ipython-input-32-108926f50545>:4) ]]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node Relu_7:\n BiasAdd_5 (defined at <ipython-input-32-108926f50545>:3)\n\nOriginal stack trace for 'Relu_7':\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-36-b56ab6a6bd42>\", line 1, in <module>\n    model = conv_net(X, weights, biases, keep_prob)\n  File \"<ipython-input-34-a415db9378fa>\", line 16, in conv_net\n    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n  File \"<ipython-input-32-108926f50545>\", line 4, in conv2d\n    return tf.nn.relu(x)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 10461, in relu\n    \"Relu\", features=features, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3QuNUVjrz37",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"alldata = []\n",
        "CNN_one_hot = np.zeros((1000, 10))\n",
        "#For each directory\n",
        "genres_map = {}\n",
        "count = 0\n",
        "count_file = 0\n",
        "for dir_name in glob.glob(\"genres/*\"):\n",
        "    genres = dir_name.split(\"/\")\n",
        "    genres = genres[len(genres)-1]\n",
        "    genres_map[count] = genres \n",
        "    #for each file in the directory\n",
        "    for file_name in glob.glob(dir_name + \"/*\"):\n",
        "        #read the file into data\n",
        "        with ar.audio_open(file_name) as f:\n",
        "            #nb_of_buffer = 0\n",
        "            #size_of_file = 0\n",
        "            data = []\n",
        "            for buf in f:\n",
        "                #nb_of_buffer+=1\n",
        "                #size_of_file+=len(buf)\n",
        "                for buf_element in buf:\n",
        "                    data.append(buf_element)\n",
        "            #size_of_buffer = size_of_file//(nb_of_buffer-1)\n",
        "            #alldata.append((data, size_of_buffer, nb_of_buffer, genres))\n",
        "            alldata.append(data)\n",
        "        CNN_one_hot[count_file, count] = 1\n",
        "        count_file+=1\n",
        "    count+=1\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1gWYx8sQdN1",
        "colab_type": "code",
        "outputId": "81b4fee1-a81c-4f49-83fa-4956adafd725",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1009
        }
      },
      "source": [
        "t = (100,'something')\n",
        "print(t[0])\n",
        "print(t[1])\n",
        "print(CNN_genres_map)\n",
        "print(CNN_genres_map_reverse)\n",
        "CNN_one_hot(data)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "something\n",
            "{0: 'rock', 1: 'metal', 2: 'hiphop', 3: 'country', 4: 'classical', 5: 'jazz', 6: 'reggae', 7: 'disco', 8: 'pop', 9: 'blues'}\n",
            "{'rock': 0, 'metal': 1, 'hiphop': 2, 'country': 3, 'classical': 4, 'jazz': 5, 'reggae': 6, 'disco': 7, 'pop': 8, 'blues': 9}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cir7e0VESi9k",
        "colab_type": "code",
        "outputId": "32741460-7e30-400c-ef8a-94b784effab6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(CNN_n_input)\n",
        "print(len(data[0][0]))\n",
        "data_reshaped = [x[0] for x in data]\n",
        "data_reshaped = [x[0:CNN_n_input] for x in data_reshaped]\n",
        "data_reshaped = np.array(data_reshaped)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1323008\n",
            "1323588\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsY5JXxagXHb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d9121893-5ff5-43e2-e528-e9a9a5209710"
      },
      "source": [
        "print(type(data_reshaped), data_reshaped.shape)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> (50, 1323008)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAH7WdD1Drfd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}